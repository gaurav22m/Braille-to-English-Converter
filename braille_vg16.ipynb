{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaurav22m/Braille-to-English-Converter/blob/main/braille_vg16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSzOrc7LSg6i"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import sklearn\n",
        "from tensorflow import keras\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3AHVP3nS0ZX",
        "outputId": "c9d9a7a6-9698-4012-e949-ed9b3890e325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dJLPmt7XS01s"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to load images from a folder\n",
        "def load_images_from_folder(folder_path):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        img_path = os.path.join(folder_path, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "    return images\n",
        "\n",
        "# Function to preprocess images\n",
        "def preprocess_images(images):\n",
        "    # Resize images to a consistent size\n",
        "    target_size = (100, 100)  # Specify the target size\n",
        "    resized_images = resize_images(images, target_size)\n",
        "\n",
        "    # Convert images to grayscale\n",
        "    grayscale_images = convert_to_grayscale(resized_images)\n",
        "\n",
        "    # Normalize pixel values\n",
        "    normalized_images = normalize_images(grayscale_images)\n",
        "\n",
        "    # Augment dataset if necessary\n",
        "    augmented_images = augment_dataset(normalized_images)\n",
        "\n",
        "    return augmented_images\n",
        "\n",
        "# Function to resize images\n",
        "def resize_images(images, target_size):\n",
        "    resized_images = []\n",
        "    for image in images:\n",
        "        resized_image = cv2.resize(image, target_size)\n",
        "        resized_images.append(resized_image)\n",
        "    return resized_images\n",
        "\n",
        "# Function to convert images to grayscale\n",
        "def convert_to_grayscale(images):\n",
        "    grayscale_images = []\n",
        "    for image in images:\n",
        "        grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        grayscale_images.append(grayscale_image)\n",
        "    return grayscale_images\n",
        "\n",
        "# Function to normalize pixel values\n",
        "def normalize_images(images):\n",
        "    normalized_images = []\n",
        "    for image in images:\n",
        "        normalized_image = image / 255.0  # Normalize pixel values to range [0, 1]\n",
        "        normalized_images.append(normalized_image)\n",
        "    return normalized_images\n",
        "\n",
        "# Function to augment dataset\n",
        "# Transformations for scaling and adding noise\n",
        "def augment_dataset(images):\n",
        "    augmented_images = []\n",
        "    scale_transform = transforms.Resize((int(1.5 * images[0].shape[0]), int(1.5 * images[0].shape[1])))\n",
        "    to_tensor_transform = transforms.ToTensor()  # Convert PIL image to tensor\n",
        "    noise_transform = transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.1)  # Add Gaussian noise\n",
        "\n",
        "    for image in images:\n",
        "        # Convert NumPy array to PIL image\n",
        "        pil_image = Image.fromarray(image)\n",
        "        # Apply transformations\n",
        "        scaled_image = scale_transform(pil_image)\n",
        "        tensor_image = to_tensor_transform(scaled_image)  # Convert PIL image to tensor\n",
        "        noisy_image = noise_transform(tensor_image)\n",
        "        # Convert back to NumPy array\n",
        "        noisy_image_np = noisy_image.permute(1, 2, 0).numpy()  # Convert tensor to NumPy array\n",
        "        # Append the augmented image to the list\n",
        "        augmented_images.append(noisy_image_np)\n",
        "\n",
        "    return augmented_images\n",
        "\n",
        "# Path to the folder containing images\n",
        "folder_path = '/content/drive/MyDrive/Braille Dataset2'\n",
        "\n",
        "# Load images from each class folder\n",
        "images_per_class = {}\n",
        "for class_folder in os.listdir(folder_path):\n",
        "    class_path = os.path.join(folder_path, class_folder)\n",
        "    if os.path.isdir(class_path):\n",
        "        images = load_images_from_folder(class_path)\n",
        "        preprocessed_images = preprocess_images(images)\n",
        "        images_per_class[class_folder] = preprocessed_images\n",
        "\n",
        "# Now 'images_per_class' dictionary contains preprocessed images for each class\n",
        "\n",
        "# Function to save preprocessed images to disk\n",
        "def save_preprocessed_images(images, output_folder):\n",
        "    for idx, image in enumerate(images):\n",
        "        output_path = os.path.join(output_folder, f\"image_{idx}.png\")\n",
        "        cv2.imwrite(output_path, (image * 255).astype(np.uint8))\n",
        "\n",
        "# Path to the folder to save preprocessed images\n",
        "output_folder = '/content/drive/MyDrive/Braille Dataset preprocessed'\n",
        "\n",
        "# Save preprocessed images for each class\n",
        "for class_name, images in images_per_class.items():\n",
        "    class_output_folder = os.path.join(output_folder, class_name)\n",
        "    os.makedirs(class_output_folder, exist_ok=True)\n",
        "    save_preprocessed_images(images, class_output_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-Jso_CQS4hM",
        "outputId": "4f449c89-ebef-4dd1-c449-3619802c0b48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set size: 858\n",
            "Validation set size: 390\n",
            "Test set size: 312\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to split dataset into train, validation, and test sets\n",
        "def split_dataset(images_per_class, test_size=0.2, validation_size=0.25, random_state=None):\n",
        "    train_images = []\n",
        "    train_labels = []\n",
        "    validation_images = []\n",
        "    validation_labels = []\n",
        "    test_images = []\n",
        "    test_labels = []\n",
        "\n",
        "    for class_name, images in images_per_class.items():\n",
        "        # Split images for each class\n",
        "        X_train, X_test, y_train, y_test = train_test_split(images, [class_name]*len(images), test_size=test_size, random_state=random_state, stratify=[class_name]*len(images))\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_size/(1-test_size), random_state=random_state, stratify=y_train)\n",
        "\n",
        "        # Add split images to respective sets\n",
        "        train_images.extend(X_train)\n",
        "        train_labels.extend(y_train)\n",
        "        validation_images.extend(X_val)\n",
        "        validation_labels.extend(y_val)\n",
        "        test_images.extend(X_test)\n",
        "        test_labels.extend(y_test)\n",
        "\n",
        "    return train_images, validation_images, test_images, train_labels, validation_labels, test_labels\n",
        "\n",
        "# Split dataset\n",
        "train_images, validation_images, test_images, train_labels, validation_labels, test_labels = split_dataset(images_per_class)\n",
        "\n",
        "# Print sizes of each set\n",
        "print(\"Train set size:\", len(train_images))\n",
        "print(\"Validation set size:\", len(validation_images))\n",
        "print(\"Test set size:\", len(test_images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq_bO5o0TKos"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "\n",
        "# Define VGG16 architecture\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self, num_classes=26):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),  # Change input channels to 1\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Define hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize model\n",
        "model = VGG16()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define batch size and number of epochs\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "\n",
        "# Define the path to the train folder\n",
        "train_folder_path = '/content/drive/MyDrive/Splitted_braille/train'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale (1 channel)\n",
        "    transforms.ToTensor(),           # Convert images to tensors\n",
        "    transforms.Normalize(mean=[0.485], std=[0.229])  # Normalize with ImageNet mean and std for grayscale images\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = datasets.ImageFolder(root=train_folder_path, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "validation_folder_path = '/content/drive/MyDrive/Splitted_braille/validation'\n",
        "if os.path.exists(validation_folder_path):\n",
        "    validation_dataset = datasets.ImageFolder(root=validation_folder_path, transform=transform)\n",
        "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size)\n",
        "else:\n",
        "    validation_loader = None\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "# Validation\n",
        "model.eval()\n",
        "val_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for inputs, labels in validation_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()  # Fix: call .sum().item() to get the total number of correct predictions\n",
        "      loss = criterion(outputs, labels)\n",
        "      val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "  # Print statistics\n",
        "  epoch_loss = running_loss / len(train_loader.dataset)\n",
        "  epoch_val_loss = val_loss / len(validation_loader.dataset)\n",
        "  accuracy = correct / total\n",
        "  print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}, Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7eYvDZZT4rV"
      },
      "outputs": [],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Lists to store true labels and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate through the validation set and make predictions\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in validation_loader:\n",
        "        inputs = inputs.double()  # Convert inputs to double\n",
        "        labels = torch.tensor(labels)  # Convert labels to tensor format\n",
        "        outputs = model(inputs.unsqueeze(1))  # Add an extra dimension for the single channel\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        true_labels.extend(labels.numpy())\n",
        "        predicted_labels.extend(predicted.numpy())\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lkaYo2MUACM"
      },
      "outputs": [],
      "source": [
        "Initialize empty lists to store precision values and epochs\n",
        "validation_precision = []\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "# Validation loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_loader:\n",
        "            inputs = inputs.double()  # Convert inputs to double\n",
        "            labels = torch.tensor(labels)  # Convert labels to tensor format\n",
        "            outputs = model(inputs.unsqueeze(1))  # Add an extra dimension for the single channel\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate precision for this epoch\n",
        "        precision = correct / total\n",
        "        validation_precision.append(precision)\n",
        "\n",
        "# Plot validation precision\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, validation_precision, marker='o', color='blue')\n",
        "plt.title('Validation Precision')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Precision')\n",
        "plt.xticks(epochs)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc-SbYgBUChi"
      },
      "outputs": [],
      "source": [
        "# Initialize empty lists to store precision and recall values for each epoch\n",
        "validation_precision = []\n",
        "validation_recall = []\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "# Validation loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    true_positives = 0\n",
        "    false_negatives = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_loader:\n",
        "            inputs = inputs.double()  # Convert inputs to double\n",
        "            labels = torch.tensor(labels)  # Convert labels to tensor format\n",
        "            outputs = model(inputs.unsqueeze(1))  # Add an extra dimension for the single channel\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            true_positives += ((predicted == labels) & (labels == 1)).sum().item()\n",
        "            false_negatives += ((predicted != labels) & (labels == 1)).sum().item()\n",
        "\n",
        "        # Calculate precision and recall for this epoch\n",
        "        precision = true_positives / (true_positives + (total - true_positives - false_negatives))\n",
        "        recall = true_positives / (true_positives + false_negatives)\n",
        "        validation_precision.append(precision)\n",
        "        validation_recall.append(recall)\n",
        "\n",
        "# Plot validation precision\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, validation_precision, marker='o', color='blue')\n",
        "plt.title('Validation Precision')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Precision')\n",
        "plt.xticks(epochs)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot validation recall\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, validation_recall, marker='o', color='green')\n",
        "plt.title('Validation Recall')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Recall')\n",
        "plt.xticks(epochs)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sVKEzpPULS7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfUr/w5/ANYI2Lg+yDPYsb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}